{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Research Assistant"
      ],
      "metadata": {
        "id": "erbqbsk-Q0QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "3CdJYt-LQ6bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install Langchain\n",
        "!pip install tiktoken\n",
        "!pip install docarray\n",
        "!pip install pypdf\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "CEIS1dCJQzgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "rUivvEJyRLw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Import required libraries."
      ],
      "metadata": {
        "id": "-fM85hX5RL0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain, LLMChain\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "WQzNNzh5tSuG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "kHwwsvJ_RW-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Functions to read flat files & pdf files."
      ],
      "metadata": {
        "id": "9a_zOr80RXCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_flat_file(file_name):\n",
        "  with open(file_name) as f:\n",
        "    file_content = f.read()\n",
        "  return file_content\n",
        "\n",
        "def read_pdf_file(file_name, chunk_size=1500, chunk_overlap=150):\n",
        "  loader = PyPDFLoader(file_path=file_name)\n",
        "  pages = loader.load()\n",
        "  splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  splits = splitter.split_documents(pages)\n",
        "  return splits"
      ],
      "metadata": {
        "id": "hLG4W7auRXSp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "teedPNvl4QA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Function to download PDF file."
      ],
      "metadata": {
        "id": "pKUlpAr-4QFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_read_pdf_file(file_url):\n",
        "  file_name = file_url.split(\"/\")[-1]\n",
        "  data = urllib.request.urlretrieve(file_url, file_name)\n",
        "  splits = read_pdf_file(file_name, chunk_size=1500, chunk_overlap=150)\n",
        "  return splits"
      ],
      "metadata": {
        "id": "XVsJRwwY4QV4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "s4PztgWgRXfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Load OpenAI keys, instantiate client objects & load embeddings."
      ],
      "metadata": {
        "id": "c2Q6us4ERXi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = read_flat_file(\"openai_key\")\n",
        "\n",
        "chatgpt3_5_turbo = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "chatgpt3_5_turbo_instruct = ChatOpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
        "chatgpt4 = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ],
      "metadata": {
        "id": "7TA6OPtkRlLR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "lHbFD2eARlXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Function to create Vector Database from document splits."
      ],
      "metadata": {
        "id": "tnwX9XnXRla-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vectordb_from_document(document_splits, embedding):\n",
        "  return  FAISS.from_documents(document_splits, embedding=embedding)"
      ],
      "metadata": {
        "id": "tjiAub0MSBXs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "wt7BEjZvSBnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Function to get Output Parsers."
      ],
      "metadata": {
        "id": "ZF6nrqU0SBrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = {\n",
        "    \"abstract\": 50,\n",
        "    \"introduction\": 300,\n",
        "    \"methodology\": 500,\n",
        "    \"results\": 200,\n",
        "    \"conclusion\": 100\n",
        "}\n",
        "\n",
        "def get_component_output_parser(component):\n",
        "  summary = ResponseSchema(name=f\"{component}\", description=f\"Very brief summary of the {component} in less than {sizes[component]} words.\")\n",
        "  return StructuredOutputParser.from_response_schemas([summary])\n",
        "\n",
        "def get_summary_output_parser():\n",
        "  summary = ResponseSchema(name=f\"summary\", description=f\"Summary of the research paper in 1000 words.\")\n",
        "  return StructuredOutputParser.from_response_schemas([summary])\n",
        "\n",
        "def get_qa_output_parser():\n",
        "  answer = ResponseSchema(name=f\"answer\", description=f\"Answer to the question asked in only one sentence.\")\n",
        "  return StructuredOutputParser.from_response_schemas([answer])"
      ],
      "metadata": {
        "id": "v1cgZqiVSRbm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "4lFdTpQGSRqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Function to get prompt templates."
      ],
      "metadata": {
        "id": "j76ZgESFSRuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_component_prompt_template():\n",
        "  prompt_template_text = \"\"\"\n",
        "  You will be provided with {component} part of a research paper enclosed within {delimiter} delimiter.\n",
        "  Please provide a summary of this {component}.\n",
        "  Please make sure that the summary is concise and to the point.\n",
        "\n",
        "  <Note>\n",
        "  The {component} part of the research paper can have some information unrealated to the {component}. You must ignore it.\n",
        "\n",
        "  <Abstract>\n",
        "\n",
        "  {delimiter}{context}{delimiter}\n",
        "\n",
        "  {instructions}\n",
        "\n",
        "  \"\"\"\n",
        "  return PromptTemplate(template=prompt_template_text, input_variables=[\"context\", \"delimiter\", \"instructions\", \"component\"])\n",
        "\n",
        "def get_qa_prompt_template():\n",
        "  prompt_template_text = \"\"\"\n",
        "  You will be provided with a question (delimited with {question_delimiter}) pertaining to a research paper.\n",
        "  You will also be provided with a relevant context (delimited with {context_delimiter}) extracted from a research paper.\n",
        "\n",
        "  Please answer the question keeping only the context in mind.\n",
        "  Answer must be one sentence long.\n",
        "\n",
        "  <Question>\n",
        "\n",
        "  {question_delimiter}{question}{question_delimiter}\n",
        "\n",
        "  <Context>\n",
        "\n",
        "  {context_delimiter}{context}{context_delimiter}\n",
        "\n",
        "  {instructions}\n",
        "\n",
        "  Let me remind again. Answer must be one sentence long.\n",
        "\n",
        "  \"\"\"\n",
        "  return PromptTemplate(template=prompt_template_text, input_variables=[\"context\", \"delimiter\", \"instructions\", \"component\"])\n",
        "\n",
        "def get_summary_prompt_template():\n",
        "  prompt_template_text = \"\"\"\n",
        "  You will be provided with details of {components_list} from a research paper enclosed within {delimiter} delimiter.\n",
        "  Please provide a combined summary from it.\n",
        "  Please make sure that the summary is concise and to the point.\n",
        "\n",
        "  {component_summary}\n",
        "\n",
        "  {instructions}\n",
        "\n",
        "  \"\"\"\n",
        "  return PromptTemplate(template=prompt_template_text, input_variables=[\"component_summary\", \"delimiter\", \"instructions\", \"components_list\"])"
      ],
      "metadata": {
        "id": "jzn_exZISR-N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "1sFm9XqXSSLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Functions to get QA & LLM chains."
      ],
      "metadata": {
        "id": "FAVjpmw5SSPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qa_chain(chat_client, prompt):\n",
        "  return load_qa_chain(llm=chat_client, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "def get_llm_chain(chat_client, prompt):\n",
        "  return LLMChain(llm=chat_client, prompt=prompt)"
      ],
      "metadata": {
        "id": "hv0dWXZTTsj6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "CxtdrsGPTszN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Functions to run QA & LLM chains."
      ],
      "metadata": {
        "id": "LPdlylTYTs2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = {\n",
        "    \"abstract\": \"abstract\",\n",
        "    \"introduction\": \"Extract the introduction section discussing background information and research objectives.\",\n",
        "    \"methodology\": \"Get the methodology section detailing experimental design, data collection, and analysis techniques.\",\n",
        "    \"results\": \"results\",\n",
        "    \"conclusion\": \"conclusion\"\n",
        "}\n",
        "\n",
        "def run_component_qa_chain(component, chat_client, vectordb):\n",
        "  query = \"What are the skills and educational qualifications of the candidate?\"\n",
        "  prompt = get_component_prompt_template()\n",
        "  output_parser = get_component_output_parser(component)\n",
        "  instructions = output_parser.get_format_instructions()\n",
        "  context = vectordb.similarity_search(query, k=1)\n",
        "  chain = get_qa_chain(chat_client, prompt)\n",
        "  prompt_inputs = {\"input_documents\": context, \"delimiter\": \"###\", \"instructions\": instructions,\n",
        "                   \"component\": component, \"words\": sizes[component]}\n",
        "  response = chain(prompt_inputs, return_only_outputs=True)\n",
        "  response_dict = output_parser.parse(response[\"output_text\"])\n",
        "  return response_dict\n",
        "\n",
        "def run_summary_llm_chain(chat_client, component_summary, components_list):\n",
        "  prompt = get_summary_prompt_template()\n",
        "  output_parser = get_summary_output_parser()\n",
        "  instructions = output_parser.get_format_instructions()\n",
        "  chain = get_llm_chain(chat_client, prompt)\n",
        "  response = chain({\"component_summary\": component_summary, \"delimiter\": \"###\", \"instructions\": instructions,\n",
        "                   \"components_list\": \", \".join(components_list)}, return_only_outputs=True)\n",
        "  response_dict = output_parser.parse(response['text'])\n",
        "  return response_dict\n",
        "\n",
        "def run_qa_chain(question, chat_client, vectordb):\n",
        "  query = question\n",
        "  prompt = get_qa_prompt_template()\n",
        "  output_parser = get_qa_output_parser()\n",
        "  instructions = output_parser.get_format_instructions()\n",
        "  context = vectordb.similarity_search(query, k=1)\n",
        "  chain = get_qa_chain(chat_client, prompt)\n",
        "  prompt_inputs = {\"input_documents\": context, \"context_delimiter\": \"###\", \"question_delimiter\": \"$$$\",\n",
        "                   \"instructions\": instructions, \"question\": question}\n",
        "  response = chain(prompt_inputs, return_only_outputs=True)\n",
        "  response_dict = output_parser.parse(response[\"output_text\"])\n",
        "  return response_dict"
      ],
      "metadata": {
        "id": "yn-LhicgTtFx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "DISLBzOvTtSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Function to get Research Paper Summary."
      ],
      "metadata": {
        "id": "RrpiomOHTtV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summary(paper_url):\n",
        "  paper = download_and_read_pdf_file(paper_url)\n",
        "  vectordb = create_vectordb_from_document(paper, embedding)\n",
        "  components = [\"abstract\", \"introduction\", \"methodology\", \"results\", \"conclusion\"]\n",
        "  summary = \"\"\n",
        "  for component in components:\n",
        "    response = run_component_qa_chain(component, chatgpt3_5_turbo, vectordb)\n",
        "    summary += \"\\n\\n\\n\\n<\" + component + \">\\n\\n\"\n",
        "    summary += \"###\" + response[component] + \"###\"\n",
        "  response = run_summary_llm_chain(chat_client=chatgpt3_5_turbo, component_summary=summary, components_list=components)\n",
        "  return response['summary'].replace(\".\", \".\\n\")"
      ],
      "metadata": {
        "id": "gO-V2ALyTtnH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "iRN6Me0zToC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Function to ask questions about research paper."
      ],
      "metadata": {
        "id": "l1dftwb6ToGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(paper_url, question, llm=chatgpt3_5_turbo):\n",
        "  paper = download_and_read_pdf_file(paper_url)\n",
        "  vectordb = create_vectordb_from_document(paper, embedding)\n",
        "  response = run_qa_chain(question, llm, vectordb)\n",
        "  return response['answer']"
      ],
      "metadata": {
        "id": "t40RJwVWToY_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "6GLmULd9Tona"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. Testing the summary application"
      ],
      "metadata": {
        "id": "BzZwEjvFPqL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_summary(paper_url=\"https://arxiv.org/pdf/1301.3781.pdf\")\n",
        "print(f\"Summary:\\n\\n{response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttuvhYJXryd1",
        "outputId": "8db51a6d-45dd-4d28-a54f-5e1ad6d1d5e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "\n",
            "The research paper presents a comprehensive test set to measure the quality of word vectors.\n",
            " The test set includes five types of semantic questions and nine types of syntactic questions.\n",
            " The semantic questions cover topics such as capital cities, currency, city-in-state relationships, man-woman relationships, and nationality adjectives.\n",
            " The syntactic questions cover topics such as adjective to adverb conversion, opposites, comparatives, superlatives, present participles, past tense, plural nouns, and plural verbs.\n",
            " The test set consists of 8869 semantic questions and 10675 syntactic questions, created by manually forming word pairs and connecting them.\n",
            " This test set can be used to evaluate the performance of word vectors in various language tasks.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_summary(paper_url=\"https://arxiv.org/pdf/2402.06525.pdf\")\n",
        "print(f\"Summary:\\n\\n{response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WBewyBhfQUD",
        "outputId": "18e3e220-3705-4795-a9fb-5146a885f9d9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "\n",
            "This research paper discusses the use of flexible infinite-width graph convolutional networks (GCNs) for representation learning.\n",
            " The authors compare GCN architectures with Gram layers and ReLU kernel non-linearity to linear layers and ReLU non-linearities.\n",
            " They investigate suitable hyperparameters for the GCN and perform grid searches for different settings.\n",
            " The paper also explores hyperparameter selection for the graph convolutional DKM in the node classification setting, including regularization and inducing Gram matrix initializations.\n",
            " Random initialization is used for numerical stability in some datasets.\n",
            " The researchers highlight the significance of representation learning in GCNs and provide insights into optimizing hyperparameters for improved performance.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "JN6nbvRnUjF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. Testing the question answer application"
      ],
      "metadata": {
        "id": "GFmOMOOAUjfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Who are the authors of this paper?"
      ],
      "metadata": {
        "id": "WdHllQT6RiAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who are the authors of this paper?\"\n",
        "paper_url=\"https://arxiv.org/pdf/1301.3781.pdf\"\n",
        "\n",
        "response = get_answer(paper_url=paper_url, question=question, llm=chatgpt3_5_turbo)\n",
        "\n",
        "print(f\"Question:\\n\\n{question}\\n\\nAnswer:\\n\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1woFStAGfQWm",
        "outputId": "510c1cf6-4152-4843-8e93-36a995664a21"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "\n",
            "Who are the authors of this paper?\n",
            "\n",
            "Answer:\n",
            "\n",
            "Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Who are the authors of this paper?\"\n",
        "paper_url=\"https://arxiv.org/pdf/2402.06525.pdf\"\n",
        "\n",
        "response = get_answer(paper_url=paper_url, question=question, llm=chatgpt3_5_turbo)\n",
        "\n",
        "print(f\"Question:\\n\\n{question}\\n\\nAnswer:\\n\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxN_6piCV7zD",
        "outputId": "88b8f5c4-10d2-4874-fb49-a80b739c176d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "\n",
            "Who are the authors of this paper?\n",
            "\n",
            "Answer:\n",
            "\n",
            "LeCun, Y ., Bengio, Y ., and Hinton, G.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "5BO1RLpNRwr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What did they accomplish in this paper?"
      ],
      "metadata": {
        "id": "Qg3ABaTvRp1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What did they accomplish in this paper?\"\n",
        "paper_url=\"https://arxiv.org/pdf/1301.3781.pdf\"\n",
        "\n",
        "response = get_answer(paper_url=paper_url, question=question, llm=chatgpt3_5_turbo)\n",
        "\n",
        "print(f\"Question:\\n\\n{question}\\n\\nAnswer:\\n\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BasDMQENfQZV",
        "outputId": "95d97e55-f8e4-4365-a8a3-352a2f5e69f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "\n",
            "What did they accomplish in this paper?\n",
            "\n",
            "Answer:\n",
            "\n",
            "In this paper, they studied the quality of word vectors and observed that high quality word vectors can be trained using simple model architectures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What did they accomplish in this paper?\"\n",
        "paper_url=\"https://arxiv.org/pdf/2402.06525.pdf\"\n",
        "\n",
        "response = get_answer(paper_url=paper_url, question=question, llm=chatgpt3_5_turbo)\n",
        "\n",
        "print(f\"Question:\\n\\n{question}\\n\\nAnswer:\\n\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQJVkyjufQcl",
        "outputId": "4809d19d-e360-4579-afa7-a9fd0b30689a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "\n",
            "What did they accomplish in this paper?\n",
            "\n",
            "Answer:\n",
            "\n",
            "This paper presents work whose goal is to advance the field of machine learning and specifically our understanding of representation learning in graphs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "Hvmg0fMsR5_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What are the limitations of this research work?"
      ],
      "metadata": {
        "id": "PFRBSGjIR6a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What are the limitations of this research work?\"\n",
        "paper_url=\"https://arxiv.org/pdf/1301.3781.pdf\"\n",
        "\n",
        "response = get_answer(paper_url=paper_url, question=question, llm=chatgpt4)\n",
        "\n",
        "print(f\"Question:\\n\\n{question}\\n\\nAnswer:\\n\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6_fgQcjcmKU",
        "outputId": "251bdf0f-60e5-4021-e096-9045e2cd6f13"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "\n",
            "What are the limitations of this research work?\n",
            "\n",
            "Answer:\n",
            "\n",
            "The research paper does not provide any explicit limitations of the work conducted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What are the limitations of this research work?\"\n",
        "paper_url=\"https://arxiv.org/pdf/2402.06525.pdf\"\n",
        "\n",
        "response = get_answer(paper_url=paper_url, question=question, llm=chatgpt4)\n",
        "\n",
        "print(f\"Question:\\n\\n{question}\\n\\nAnswer:\\n\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_invFFsRBC_",
        "outputId": "00ecf025-5498-43f7-e4c1-0a03163bdf68"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "\n",
            "What are the limitations of this research work?\n",
            "\n",
            "Answer:\n",
            "\n",
            "The limitations of this research work are the high computational cost of the graph DKMs despite using an inducing point scheme, and the need for more careful optimization to aid performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "652T8asKRVSE"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}