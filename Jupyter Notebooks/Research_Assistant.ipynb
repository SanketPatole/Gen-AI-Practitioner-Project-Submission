{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Research Assistant"
      ],
      "metadata": {
        "id": "erbqbsk-Q0QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "3CdJYt-LQ6bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install Langchain\n",
        "!pip install tiktoken\n",
        "!pip install docarray\n",
        "!pip install pypdf\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "CEIS1dCJQzgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "rUivvEJyRLw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Import required libraries."
      ],
      "metadata": {
        "id": "-fM85hX5RL0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain, LLMChain\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "WQzNNzh5tSuG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "kHwwsvJ_RW-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Functions to read flat files & pdf files."
      ],
      "metadata": {
        "id": "9a_zOr80RXCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_flat_file(file_name):\n",
        "  with open(file_name) as f:\n",
        "    file_content = f.read()\n",
        "  return file_content\n",
        "\n",
        "def read_pdf_file(file_name, chunk_size=1500, chunk_overlap=150):\n",
        "  loader = PyPDFLoader(file_path=file_name)\n",
        "  pages = loader.load()\n",
        "  splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  splits = splitter.split_documents(pages)\n",
        "  return splits"
      ],
      "metadata": {
        "id": "hLG4W7auRXSp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "teedPNvl4QA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Function to download PDF file."
      ],
      "metadata": {
        "id": "pKUlpAr-4QFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_read_pdf_file(file_url):\n",
        "  file_name = file_url.split(\"/\")[-1]\n",
        "  data = urllib.request.urlretrieve(file_url, file_name)\n",
        "  splits = read_pdf_file(file_name, chunk_size=1500, chunk_overlap=150)\n",
        "  return splits"
      ],
      "metadata": {
        "id": "XVsJRwwY4QV4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "s4PztgWgRXfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Load OpenAI keys, instantiate client objects & load embeddings."
      ],
      "metadata": {
        "id": "c2Q6us4ERXi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = read_flat_file(\"openai_key\")\n",
        "\n",
        "chatgpt3_5_turbo = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "chatgpt3_5_turbo_instruct = ChatOpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
        "chatgpt4 = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ],
      "metadata": {
        "id": "7TA6OPtkRlLR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "lHbFD2eARlXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Function to create Vector Database from document splits."
      ],
      "metadata": {
        "id": "tnwX9XnXRla-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vectordb_from_document(document_splits, embedding):\n",
        "  return  FAISS.from_documents(document_splits, embedding=embedding)"
      ],
      "metadata": {
        "id": "tjiAub0MSBXs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "wt7BEjZvSBnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Function to get Output Parsers."
      ],
      "metadata": {
        "id": "ZF6nrqU0SBrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = {\n",
        "    \"abstract\": 50,\n",
        "    \"introduction\": 300,\n",
        "    \"methodology\": 500,\n",
        "    \"results\": 200,\n",
        "    \"conclusion\": 100\n",
        "}\n",
        "\n",
        "def get_component_output_parser(component):\n",
        "  summary = ResponseSchema(name=f\"{component}\", description=f\"Very brief summary of the {component} in less than {sizes[component]} words.\")\n",
        "  return StructuredOutputParser.from_response_schemas([summary])\n",
        "\n",
        "def get_summary_output_parser():\n",
        "  summary = ResponseSchema(name=f\"summary\", description=f\"Summary of the research paper in 1000 words.\")\n",
        "  return StructuredOutputParser.from_response_schemas([summary])\n",
        "\n",
        "def get_qa_output_parser():\n",
        "  answer = ResponseSchema(name=f\"answer\", description=f\"Answer to the question asked in only one sentence.\")\n",
        "  return StructuredOutputParser.from_response_schemas([answer])"
      ],
      "metadata": {
        "id": "v1cgZqiVSRbm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "4lFdTpQGSRqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Function to get prompt templates."
      ],
      "metadata": {
        "id": "j76ZgESFSRuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_component_prompt_template():\n",
        "  prompt_template_text = \"\"\"\n",
        "  You will be provided with {component} part of a research paper enclosed within {delimiter} delimiter.\n",
        "  Please provide a summary of this {component}.\n",
        "  Please make sure that the summary is concise and to the point.\n",
        "\n",
        "  <Note>\n",
        "  The {component} part of the research paper can have some information unrealated to the {component}. You must ignore it.\n",
        "\n",
        "  <Abstract>\n",
        "\n",
        "  {delimiter}{context}{delimiter}\n",
        "\n",
        "  {instructions}\n",
        "\n",
        "  \"\"\"\n",
        "  return PromptTemplate(template=prompt_template_text, input_variables=[\"context\", \"delimiter\", \"instructions\", \"component\"])\n",
        "\n",
        "def get_qa_prompt_template():\n",
        "  prompt_template_text = \"\"\"\n",
        "  You will be provided with a question (delimited with {question_delimiter}) pertaining to a research paper.\n",
        "  You will also be provided with a relevant context (delimited with {context_delimiter}) extracted from a research paper.\n",
        "\n",
        "  Please answer the question keeping only the context in mind.\n",
        "  Answer must be one sentence long.\n",
        "\n",
        "  <Question>\n",
        "\n",
        "  {question_delimiter}{question}{question_delimiter}\n",
        "\n",
        "  <Context>\n",
        "\n",
        "  {context_delimiter}{context}{context_delimiter}\n",
        "\n",
        "  {instructions}\n",
        "\n",
        "  Let me remind again. Answer must be one sentence long.\n",
        "\n",
        "  \"\"\"\n",
        "  return PromptTemplate(template=prompt_template_text, input_variables=[\"context\", \"delimiter\", \"instructions\", \"component\"])\n",
        "\n",
        "def get_summary_prompt_template():\n",
        "  prompt_template_text = \"\"\"\n",
        "  You will be provided with details of {components_list} from a research paper enclosed within {delimiter} delimiter.\n",
        "  Please provide a combined summary from it.\n",
        "  Please make sure that the summary is concise and to the point.\n",
        "\n",
        "  {component_summary}\n",
        "\n",
        "  {instructions}\n",
        "\n",
        "  \"\"\"\n",
        "  return PromptTemplate(template=prompt_template_text, input_variables=[\"component_summary\", \"delimiter\", \"instructions\", \"components_list\"])"
      ],
      "metadata": {
        "id": "jzn_exZISR-N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "1sFm9XqXSSLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Functions to get QA & LLM chains."
      ],
      "metadata": {
        "id": "FAVjpmw5SSPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qa_chain(chat_client, prompt):\n",
        "  return load_qa_chain(llm=chat_client, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "def get_llm_chain(chat_client, prompt):\n",
        "  return LLMChain(llm=chat_client, prompt=prompt)"
      ],
      "metadata": {
        "id": "hv0dWXZTTsj6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "CxtdrsGPTszN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Functions to run QA & LLM chains."
      ],
      "metadata": {
        "id": "LPdlylTYTs2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = {\n",
        "    \"abstract\": \"abstract\",\n",
        "    \"introduction\": \"Extract the introduction section discussing background information and research objectives.\",\n",
        "    \"methodology\": \"Get the methodology section detailing experimental design, data collection, and analysis techniques.\",\n",
        "    \"results\": \"results\",\n",
        "    \"conclusion\": \"conclusion\"\n",
        "}\n",
        "\n",
        "def run_component_qa_chain(component, chat_client, vectordb):\n",
        "  query = \"What are the skills and educational qualifications of the candidate?\"\n",
        "  prompt = get_component_prompt_template()\n",
        "  output_parser = get_component_output_parser(component)\n",
        "  instructions = output_parser.get_format_instructions()\n",
        "  context = vectordb.similarity_search(query, k=1)\n",
        "  chain = get_qa_chain(chat_client, prompt)\n",
        "  prompt_inputs = {\"input_documents\": context, \"delimiter\": \"###\", \"instructions\": instructions,\n",
        "                   \"component\": component, \"words\": sizes[component]}\n",
        "  response = chain(prompt_inputs, return_only_outputs=True)\n",
        "  response_dict = output_parser.parse(response[\"output_text\"])\n",
        "  return response_dict\n",
        "\n",
        "def run_summary_llm_chain(chat_client, component_summary, components_list):\n",
        "  prompt = get_summary_prompt_template()\n",
        "  output_parser = get_summary_output_parser()\n",
        "  instructions = output_parser.get_format_instructions()\n",
        "  chain = get_llm_chain(chat_client, prompt)\n",
        "  response = chain({\"component_summary\": component_summary, \"delimiter\": \"###\", \"instructions\": instructions,\n",
        "                   \"components_list\": \", \".join(components_list)}, return_only_outputs=True)\n",
        "  response_dict = output_parser.parse(response['text'])\n",
        "  return response_dict\n",
        "\n",
        "def run_qa_chain(question, chat_client, vectordb):\n",
        "  query = question\n",
        "  prompt = get_qa_prompt_template()\n",
        "  output_parser = get_qa_output_parser()\n",
        "  instructions = output_parser.get_format_instructions()\n",
        "  context = vectordb.similarity_search(query, k=1)\n",
        "  chain = get_qa_chain(chat_client, prompt)\n",
        "  prompt_inputs = {\"input_documents\": context, \"context_delimiter\": \"###\", \"question_delimiter\": \"$$$\",\n",
        "                   \"instructions\": instructions, \"question\": question}\n",
        "  response = chain(prompt_inputs, return_only_outputs=True)\n",
        "  response_dict = output_parser.parse(response[\"output_text\"])\n",
        "  return response_dict"
      ],
      "metadata": {
        "id": "yn-LhicgTtFx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "DISLBzOvTtSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Function to get Research Paper Summary."
      ],
      "metadata": {
        "id": "RrpiomOHTtV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summary(paper_url):\n",
        "  paper = download_and_read_pdf_file(paper_url)\n",
        "  vectordb = create_vectordb_from_document(paper, embedding)\n",
        "  components = [\"abstract\", \"introduction\", \"methodology\", \"results\", \"conclusion\"]\n",
        "  summary = \"\"\n",
        "  for component in components:\n",
        "    response = run_component_qa_chain(component, chatgpt3_5_turbo, vectordb)\n",
        "    summary += \"\\n\\n\\n\\n<\" + component + \">\\n\\n\"\n",
        "    summary += \"###\" + response[component] + \"###\"\n",
        "  response = run_summary_llm_chain(chat_client=chatgpt3_5_turbo, component_summary=summary, components_list=components)\n",
        "  return response['summary'].replace(\".\", \".\\n\")"
      ],
      "metadata": {
        "id": "gO-V2ALyTtnH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "iRN6Me0zToC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Function to ask questions about research paper."
      ],
      "metadata": {
        "id": "l1dftwb6ToGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(paper_url, question, llm=chatgpt3_5_turbo):\n",
        "  paper = download_and_read_pdf_file(paper_url)\n",
        "  vectordb = create_vectordb_from_document(paper, embedding)\n",
        "  response = run_qa_chain(question, llm, vectordb)\n",
        "  return response['answer']"
      ],
      "metadata": {
        "id": "t40RJwVWToY_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "6GLmULd9Tona"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. Testing the summary application"
      ],
      "metadata": {
        "id": "BzZwEjvFPqL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_summary(paper_url=\"https://arxiv.org/pdf/1301.3781.pdf\")\n",
        "print(\"Summary:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttuvhYJXryd1",
        "outputId": "05cf6b92-be47-4790-f1b4-d647771442e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "\n",
            "The research paper presents a comprehensive test set consisting of five types of semantic questions and nine types of syntactic questions.\n",
            " The test set includes examples such as word pairs related to capital cities, currencies, city-state relationships, and word forms.\n",
            " The quality of word vectors is measured using this test set, which contains a total of 8869 semantic questions and 10675 syntactic questions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_summary(paper_url=\"https://arxiv.org/pdf/2402.06525.pdf\")\n",
        "print(\"Summary:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WBewyBhfQUD",
        "outputId": "5993ea5d-8a83-4fbe-ff09-aa0a0adca1c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "\n",
            "The research paper discusses the use of flexible infinite-width graph convolutional networks (GCNs) for representation learning and the importance of finding suitable hyperparameters for these networks.\n",
            " The authors propose using Gram layers and the ReLU kernel non-linearity instead of linear layers and ReLU non-linearities in GCN architectures.\n",
            " They also discuss the use of global kernel norm instead of batch normalization.\n",
            " The authors conducted grid searches to determine suitable hyperparameters for the GCN and graph convolutional DKM models.\n",
            " They experimented with different settings such as hidden units, dropout, batch normalization, row normalization, regularization, number of layers, inducing points, and Gram matrix initializations.\n",
            " The paper highlights the importance of using random initialization for certain datasets to ensure numerical stability.\n",
            " Overall, the research paper emphasizes the significance of representation learning in graph convolutional networks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "JN6nbvRnUjF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. Testing the question answer application"
      ],
      "metadata": {
        "id": "GFmOMOOAUjfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_answer(paper_url=\"https://arxiv.org/pdf/1301.3781.pdf\",\n",
        "                     question=\"Who are the authors of this paper?\",\n",
        "                     llm=chatgpt3_5_turbo)\n",
        "print(\"Answer:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1woFStAGfQWm",
        "outputId": "4e410895-c178-4f46-d284-4b477743d912"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "\n",
            "Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_answer(paper_url=\"https://arxiv.org/pdf/1301.3781.pdf\",\n",
        "                     question=\"What did they accomplish in this paper?\",\n",
        "                     llm=chatgpt3_5_turbo)\n",
        "print(\"Answer:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxN_6piCV7zD",
        "outputId": "abdde3eb-2b14-427a-8061-ee3b7036d8c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "\n",
            "In this paper, they studied the quality of word vectors derived by various models on a collection of syntactic and semantic language tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_answer(paper_url=\"https://arxiv.org/pdf/2402.06525.pdf\",\n",
        "                     question=\"Who are the authors of this paper?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BasDMQENfQZV",
        "outputId": "0b42dc1f-88e5-4f21-c7e1-41829b9c3432"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeCun, Y ., Bengio, Y ., and Hinton, G.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_answer(paper_url=\"https://arxiv.org/pdf/2402.06525.pdf\",\n",
        "                     question=\"What did they accomplish in this paper?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQJVkyjufQcl",
        "outputId": "072920dd-6a9c-440e-a2b6-42c8677264b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This paper aims to advance the field of machine learning and understanding of representation learning in graphs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6_fgQcjcmKU"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}